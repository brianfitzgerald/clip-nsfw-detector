{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install clip-retrieval img2dataset pandas numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (4.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (2.29.0)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (1.13.1+cu117)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision>=0.10.1,<2 --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\coold\\Documents\\GitHub\\clip-nsfw-detector\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from clip_retrieval import clip_inference\n",
    "import shutil\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "base_folder = os.path.join(\"F:\", \"nude_sexy_safe_v1_x320\", \"testing\")\n",
    "output_base_folder = os.path.join(\"F:\", \"nsfw_embeddings\")\n",
    "categories = [\"nude\", \"sexy\", \"safe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples has been estimated to be 3878\n",
      "Starting the worker\n",
      "dataset is 37\n",
      "Starting work on task 0\n",
      "warming up with batch size 32 on cuda\n",
      "done warming up in 3.021339178085327s\n",
      "The number of samples has been estimated to be 2121\n",
      "Starting the worker\n",
      "dataset is 37\n",
      "Starting work on task 0\n",
      "The number of samples has been estimated to be 4050\n",
      "Starting the worker\n",
      "dataset is 37\n",
      "Starting work on task 0\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(categories):\n",
    "    folder = os.path.join(base_folder, c)\n",
    "    output_folder = os.path.join(output_base_folder, c)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    # https://github.com/rom1504/clip-retrieval/issues/220\n",
    "    clip_inference(folder, output_folder, num_prepro_workers=0, clip_model=\"ViT-L/14\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embeddings = {}\n",
    "for c in categories:\n",
    "    cat_embeddings[c] = np.load(os.path.join(output_base_folder, c, \"img_emb\", \"img_emb_0.npy\"))\n",
    "\n",
    "pos_x = np.concatenate([cat_embeddings[\"nude\"], cat_embeddings[\"sexy\"]])\n",
    "neg_x = cat_embeddings[\"safe\"]\n",
    "\n",
    "num_pos = pos_x.shape[0]\n",
    "num_neg = neg_x.shape[0]\n",
    "\n",
    "pos_y = np.ones((num_pos, 1))\n",
    "neg_y = np.zeros((num_neg, 1))\n",
    "\n",
    "x_np = np.vstack([pos_x, neg_x])\n",
    "y_np = np.vstack([pos_y, neg_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16 torch.float16\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(x_np).type(torch.float16)\n",
    "y = torch.from_numpy(y_np).type(torch.float16)\n",
    "print(x.dtype, y.dtype)\n",
    "train_dataset = TensorDataset(x, y)\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([853, 768])\n"
     ]
    }
   ],
   "source": [
    "baseline = torch.from_numpy(np.load(r\"F:\\nsfw_testset\\nsfw_testset\\drawings-test\\img_emb\\img_emb_0.npy\"))\n",
    "print(baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:crmrarzh) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>▇▆▄▂▃█▃▆▇▇▃▄▂▅▅▆▆▇▆▅▇▅▇▇▆▃█▆▇▅▅▆▄█▆▅▅▄▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>loss</td><td>0.02549</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">lucky-dust-1</strong>: <a href=\"https://wandb.ai/bfitzgerald/nsfw-detefctor/runs/crmrarzh\" target=\"_blank\">https://wandb.ai/bfitzgerald/nsfw-detefctor/runs/crmrarzh</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230430_123526-crmrarzh\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:crmrarzh). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.0 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\coold\\Documents\\GitHub\\clip-nsfw-detector\\wandb\\run-20230430_123613-8py8xhnl</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bfitzgerald/nsfw-detefctor/runs/8py8xhnl\" target=\"_blank\">peachy-pyramid-2</a></strong> to <a href=\"https://wandb.ai/bfitzgerald/nsfw-detefctor\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:01<00:00, 175.19it/s, epoch=0, loss=0.544]\n",
      "100%|██████████| 315/315 [00:01<00:00, 195.77it/s, epoch=1, loss=0.0733]\n",
      "100%|██████████| 315/315 [00:01<00:00, 187.84it/s, epoch=2, loss=0.527]\n",
      "100%|██████████| 315/315 [00:01<00:00, 170.64it/s, epoch=3, loss=0.533]\n",
      "100%|██████████| 315/315 [00:01<00:00, 171.48it/s, epoch=4, loss=0.536]\n",
      "100%|██████████| 315/315 [00:01<00:00, 164.35it/s, epoch=5, loss=0.535]\n",
      "100%|██████████| 315/315 [00:01<00:00, 160.01it/s, epoch=6, loss=0.0716]\n",
      "100%|██████████| 315/315 [00:01<00:00, 161.13it/s, epoch=7, loss=0.534]\n",
      "100%|██████████| 315/315 [00:01<00:00, 169.93it/s, epoch=8, loss=0.544]\n",
      "100%|██████████| 315/315 [00:01<00:00, 161.79it/s, epoch=9, loss=0.536]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, AdamW\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "class H14_NSFW_Detector(nn.Module):\n",
    "    def __init__(self, input_size=1024):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = H14_NSFW_Detector(768).to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = AdamW(model.parameters())\n",
    "num_epochs = 100\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "wandb.init(project=\"nsfw-detefctor\")\n",
    "wandb.watch(model)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_iter = tqdm(train_loader, total=len(train_loader))\n",
    "        for inputs, labels in train_iter:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_iter.set_postfix(loss=loss.item(), epoch=epoch)\n",
    "            wandb.log({\"loss\": loss.item(), \"epoch\": epoch, \"lr\": optimizer.param_groups[0]['lr']})\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
