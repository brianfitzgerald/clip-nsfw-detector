{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install clip-retrieval img2dataset pandas numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the file specified.\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision>=0.10.1,<2 --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from clip_retrieval import clip_inference\n",
    "import shutil\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "base_folder = os.path.join(\"F:\", \"nude_sexy_safe_v1_x320\", \"validation\")\n",
    "output_base_folder = os.path.join(\"F:\", \"nsfw_embeddings\")\n",
    "categories = [\"nude\", \"sexy\", \"safe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples has been estimated to be 4000\n",
      "Starting the worker\n",
      "dataset is 40\n",
      "Starting work on task 0\n",
      "The number of samples has been estimated to be 2248\n",
      "Starting the worker\n",
      "dataset is 40\n",
      "Starting work on task 0\n",
      "The number of samples has been estimated to be 4058\n",
      "Starting the worker\n",
      "dataset is 40\n",
      "Starting work on task 0\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(categories):\n",
    "    folder = os.path.join(base_folder, c)\n",
    "    output_folder = os.path.join(output_base_folder, c)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    # https://github.com/rom1504/clip-retrieval/issues/220\n",
    "    clip_inference(folder, output_folder, num_prepro_workers=0, clip_model=\"ViT-L/14\", batch_size=128, enable_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embeddings = {}\n",
    "for c in categories:\n",
    "    cat_embeddings[c] = np.load(os.path.join(output_base_folder, c, \"img_emb\", \"img_emb_0.npy\"))\n",
    "\n",
    "\n",
    "def npy_to_tensor_dataset(pos_x, neg_x):\n",
    "\n",
    "    max_len = min(pos_x.shape[0], neg_x.shape[0])\n",
    "    pos_x, neg_x = pos_x[:max_len], neg_x[:max_len]\n",
    "\n",
    "    num_pos = pos_x.shape[0]\n",
    "    num_neg = neg_x.shape[0]\n",
    "\n",
    "    pos_y = np.ones((num_pos, 1))\n",
    "    neg_y = np.zeros((num_neg, 1))\n",
    "\n",
    "    x_np = np.vstack((pos_x, neg_x))\n",
    "    y_np = np.vstack((pos_y, neg_y))\n",
    "\n",
    "    return torch.from_numpy(x_np).type(torch.float16), torch.from_numpy(y_np).type(torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_x = torch.from_numpy(np.load(r\"F:\\nsfw_testset\\drawings-test\\img_emb\\img_emb_0.npy\"))\n",
    "# neg_x = torch.from_numpy(np.load(r\"F:\\nsfw_testset\\neutral-test\\img_emb\\img_emb_0.npy\"))\n",
    "\n",
    "pos_x_val = np.concatenate([cat_embeddings[\"nude\"], cat_embeddings[\"sexy\"]])\n",
    "neg_x_val = cat_embeddings[\"safe\"]\n",
    "\n",
    "x_val, y_val = npy_to_tensor_dataset(pos_x_val, neg_x_val)\n",
    "\n",
    "train_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\coold\\Documents\\GitHub\\clip-nsfw-detector\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1143: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0.991514360313316\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import numpy as np\n",
    "from joblib import dump, load\n",
    "\n",
    "pos_x_val = np.concatenate([cat_embeddings[\"nude\"], cat_embeddings[\"sexy\"]])\n",
    "neg_x_val = cat_embeddings[\"safe\"]\n",
    "\n",
    "clf = SVC()\n",
    "# clf = AdaBoostClassifier()\n",
    "clf.fit(x_val, y_val)\n",
    "x_val, y_val = npy_to_tensor_dataset(pos_x_val, neg_x_val)\n",
    "\n",
    "pos_val_categories = [\"porn-test\", \"sexy-test\", \"drawings-test\", \"hentai-test\", \"neutral-test\"]\n",
    "pos_x_val = []\n",
    "for c in pos_val_categories:\n",
    "    pos_x_val.append(torch.from_numpy(np.load(os.path.join(r\"F:\", \"nsfw_testset\", c, \"img_emb\", \"img_emb_0.npy\"))))\n",
    "pos_x_val = torch.cat(pos_x_val)\n",
    "neg_x_val = torch.from_numpy(np.load(r\"F:\\nsfw_testset\\neutral-test\\img_emb\\img_emb_0.npy\"))\n",
    "\n",
    "x_val, y_val = npy_to_tensor_dataset(pos_x_val, neg_x_val)\n",
    "y_val_list = y_val.squeeze().tolist()\n",
    "\n",
    "vals = clf.predict(x_val)\n",
    "print(y_val_list)\n",
    "print(list(vals))\n",
    "acc = np.sum(vals == y_val_list) / len(vals)\n",
    "print(acc)\n",
    "\n",
    "dump(clf, r\"F:\\nsfw_testset\\nsfw_classifier.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 254/254 [00:01<00:00, 149.09it/s, log={'loss': 1.6469610929489136, 'epoch': 0, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 174.31it/s, log={'loss': 1.38545823097229, 'epoch': 1, 'lr': 0.0001}]   \n",
      "100%|██████████| 254/254 [00:01<00:00, 174.09it/s, log={'loss': 1.0441330671310425, 'epoch': 2, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 168.77it/s, log={'loss': 0.9124696850776672, 'epoch': 3, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 173.24it/s, log={'loss': 0.735766589641571, 'epoch': 4, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 175.29it/s, log={'loss': 0.7718594670295715, 'epoch': 5, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 175.78it/s, log={'loss': 0.8509055972099304, 'epoch': 6, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 178.12it/s, log={'loss': 0.9851099252700806, 'epoch': 7, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 179.33it/s, log={'loss': 1.397853136062622, 'epoch': 8, 'lr': 0.0001}]  \n",
      "100%|██████████| 254/254 [00:01<00:00, 179.17it/s, log={'loss': 0.6385857462882996, 'epoch': 9, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 176.14it/s, log={'loss': 0.691017746925354, 'epoch': 10, 'lr': 0.0001}]  \n",
      "100%|██████████| 254/254 [00:01<00:00, 175.87it/s, log={'loss': 0.6028923392295837, 'epoch': 11, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 174.93it/s, log={'loss': 0.6349191665649414, 'epoch': 12, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 177.62it/s, log={'loss': 0.6542213559150696, 'epoch': 13, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 177.37it/s, log={'loss': 0.8175692558288574, 'epoch': 14, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 178.10it/s, log={'loss': 0.42425987124443054, 'epoch': 15, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 179.25it/s, log={'loss': 0.36011621356010437, 'epoch': 16, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 176.63it/s, log={'loss': 0.4339721202850342, 'epoch': 17, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 165.94it/s, log={'loss': 0.36109089851379395, 'epoch': 18, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 165.69it/s, log={'loss': 0.3544805943965912, 'epoch': 19, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 158.45it/s, log={'loss': 0.46727925539016724, 'epoch': 20, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 162.47it/s, log={'loss': 0.7459635734558105, 'epoch': 21, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 173.62it/s, log={'loss': 0.3875425457954407, 'epoch': 22, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 170.85it/s, log={'loss': 0.41629138588905334, 'epoch': 23, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 175.05it/s, log={'loss': 0.34781453013420105, 'epoch': 24, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 179.80it/s, log={'loss': 0.3956916630268097, 'epoch': 25, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 171.74it/s, log={'loss': 0.2638028860092163, 'epoch': 26, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 167.66it/s, log={'loss': 0.30065956711769104, 'epoch': 27, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 174.44it/s, log={'loss': 0.3052411377429962, 'epoch': 28, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 174.09it/s, log={'loss': 0.29788556694984436, 'epoch': 29, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 168.44it/s, log={'loss': 0.21380217373371124, 'epoch': 30, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 175.63it/s, log={'loss': 0.36642852425575256, 'epoch': 31, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 176.32it/s, log={'loss': 0.34773293137550354, 'epoch': 32, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 175.05it/s, log={'loss': 0.224983811378479, 'epoch': 33, 'lr': 0.0001}]  \n",
      "100%|██████████| 254/254 [00:01<00:00, 176.76it/s, log={'loss': 0.19476862251758575, 'epoch': 34, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 155.90it/s, log={'loss': 0.3142032325267792, 'epoch': 35, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 159.35it/s, log={'loss': 0.36649537086486816, 'epoch': 36, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 165.58it/s, log={'loss': 0.28427979350090027, 'epoch': 37, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 164.28it/s, log={'loss': 0.185128316283226, 'epoch': 38, 'lr': 0.0001}]  \n",
      "100%|██████████| 254/254 [00:01<00:00, 164.94it/s, log={'loss': 0.20445959270000458, 'epoch': 39, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 170.81it/s, log={'loss': 0.2832636535167694, 'epoch': 40, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 167.42it/s, log={'loss': 0.31455913186073303, 'epoch': 41, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 159.25it/s, log={'loss': 0.3478493392467499, 'epoch': 42, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 178.02it/s, log={'loss': 0.1977422684431076, 'epoch': 43, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 160.96it/s, log={'loss': 0.2640240490436554, 'epoch': 44, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 157.79it/s, log={'loss': 0.3130151033401489, 'epoch': 45, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 167.22it/s, log={'loss': 0.3219846785068512, 'epoch': 46, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 172.79it/s, log={'loss': 0.31281429529190063, 'epoch': 47, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 166.54it/s, log={'loss': 0.1419353485107422, 'epoch': 48, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 173.18it/s, log={'loss': 0.19508811831474304, 'epoch': 49, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 173.73it/s, log={'loss': 0.11956112831830978, 'epoch': 50, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 177.28it/s, log={'loss': 0.12674610316753387, 'epoch': 51, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 174.27it/s, log={'loss': 0.288363516330719, 'epoch': 52, 'lr': 0.0001}]  \n",
      "100%|██████████| 254/254 [00:01<00:00, 167.66it/s, log={'loss': 0.15861153602600098, 'epoch': 53, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 164.19it/s, log={'loss': 0.1026025041937828, 'epoch': 54, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 164.34it/s, log={'loss': 0.23210182785987854, 'epoch': 55, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 167.77it/s, log={'loss': 0.1759347915649414, 'epoch': 56, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 169.56it/s, log={'loss': 0.1296701282262802, 'epoch': 57, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 170.50it/s, log={'loss': 0.2284564971923828, 'epoch': 58, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 173.14it/s, log={'loss': 0.25607845187187195, 'epoch': 59, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 174.33it/s, log={'loss': 0.14881117641925812, 'epoch': 60, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 172.84it/s, log={'loss': 0.17632925510406494, 'epoch': 61, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 175.08it/s, log={'loss': 0.1555241346359253, 'epoch': 62, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 168.21it/s, log={'loss': 0.18205364048480988, 'epoch': 63, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 168.77it/s, log={'loss': 0.09428055584430695, 'epoch': 64, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 168.24it/s, log={'loss': 0.14804920554161072, 'epoch': 65, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 166.89it/s, log={'loss': 0.15764783322811127, 'epoch': 66, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 165.80it/s, log={'loss': 0.3070368468761444, 'epoch': 67, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 172.56it/s, log={'loss': 0.1409807652235031, 'epoch': 68, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 171.85it/s, log={'loss': 0.17444616556167603, 'epoch': 69, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 173.50it/s, log={'loss': 0.15381024777889252, 'epoch': 70, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 170.51it/s, log={'loss': 0.10712303966283798, 'epoch': 71, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 167.81it/s, log={'loss': 0.08577924221754074, 'epoch': 72, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 167.88it/s, log={'loss': 0.10147812217473984, 'epoch': 73, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 164.29it/s, log={'loss': 0.10724081099033356, 'epoch': 74, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 169.59it/s, log={'loss': 0.13057665526866913, 'epoch': 75, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 171.04it/s, log={'loss': 0.10578271001577377, 'epoch': 76, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 174.17it/s, log={'loss': 0.13821467757225037, 'epoch': 77, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 174.08it/s, log={'loss': 0.1166170984506607, 'epoch': 78, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 170.34it/s, log={'loss': 0.22825880348682404, 'epoch': 79, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 165.47it/s, log={'loss': 0.17737522721290588, 'epoch': 80, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 169.60it/s, log={'loss': 0.06664111465215683, 'epoch': 81, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 168.59it/s, log={'loss': 0.09162568300962448, 'epoch': 82, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 164.19it/s, log={'loss': 0.13924431800842285, 'epoch': 83, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 167.96it/s, log={'loss': 0.13020861148834229, 'epoch': 84, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 168.04it/s, log={'loss': 0.14271870255470276, 'epoch': 85, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 174.45it/s, log={'loss': 0.08343946188688278, 'epoch': 86, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 171.76it/s, log={'loss': 0.19439469277858734, 'epoch': 87, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 168.58it/s, log={'loss': 0.0961921364068985, 'epoch': 88, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 165.08it/s, log={'loss': 0.0881006047129631, 'epoch': 89, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 168.32it/s, log={'loss': 0.15514978766441345, 'epoch': 90, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 166.82it/s, log={'loss': 0.0678422600030899, 'epoch': 91, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 160.40it/s, log={'loss': 0.07911424338817596, 'epoch': 92, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 167.11it/s, log={'loss': 0.11469130963087082, 'epoch': 93, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 170.45it/s, log={'loss': 0.09751853346824646, 'epoch': 94, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 167.55it/s, log={'loss': 0.13690395653247833, 'epoch': 95, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 167.99it/s, log={'loss': 0.12119026482105255, 'epoch': 96, 'lr': 0.0001}]\n",
      "100%|██████████| 254/254 [00:01<00:00, 166.56it/s, log={'loss': 0.2681848704814911, 'epoch': 97, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 166.21it/s, log={'loss': 0.13758496940135956, 'epoch': 98, 'lr': 0.0001}] \n",
      "100%|██████████| 254/254 [00:01<00:00, 163.45it/s, log={'loss': 0.0935506597161293, 'epoch': 99, 'lr': 0.0001}] \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, AdamW, Adagrad\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "class H14_NSFW_Detector(nn.Module):\n",
    "    def __init__(self, input_size=1024):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(2048),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = H14_NSFW_Detector(768).to(device)\n",
    "\n",
    "for layer in model.layers:\n",
    "    if isinstance(layer, nn.Linear):\n",
    "        nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "model.train()\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "num_epochs = 100\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "# wandb.init(project=\"nsfw-detefctor\")\n",
    "# wandb.watch(model)\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_iter = tqdm(train_loader, total=len(train_loader))\n",
    "        for inputs, labels in train_iter:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            inputs_list, labels_list = inputs.tolist(), labels.tolist()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            outputs_list = outputs.tolist()\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            log_dict = {\"loss\": loss.item(), \"epoch\": epoch, \"lr\": optimizer.param_groups[0]['lr']}\n",
    "            train_iter.set_postfix(log=log_dict)\n",
    "            # wandb.log(log_dict)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(768,)\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "import urllib\n",
    "import io\n",
    "from PIL import Image as pimage\n",
    "\n",
    "model, preprocess = clip.load(\"ViT-L/14\", device=\"cpu\", jit=True)\n",
    "\n",
    "def download_image(url):\n",
    "    urllib_request = urllib.request.Request(\n",
    "        url,\n",
    "        data=None,\n",
    "        headers={\"User-Agent\": \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:72.0) Gecko/20100101 Firefox/72.0\"},\n",
    "    )\n",
    "    with urllib.request.urlopen(urllib_request, timeout=10) as r:\n",
    "        img_stream = io.BytesIO(r.read())\n",
    "    return img_stream\n",
    "\n",
    "def get_image_emb(image_url):\n",
    "    with torch.no_grad():\n",
    "        image = pimage.open(download_image(image_url))\n",
    "        image_emb = model.encode_image(preprocess(image).unsqueeze(0).to(\"cpu\"))\n",
    "        image_emb /= image_emb.norm(dim=-1, keepdim=True)\n",
    "        image_emb = image_emb.cpu().detach().numpy().astype(\"float32\")[0]\n",
    "        return image_emb\n",
    "\n",
    "def get_text_emb(text):\n",
    "    with torch.no_grad():\n",
    "        text_emb = model.encode_text(clip.tokenize([text], truncate=True).to(\"cpu\"))\n",
    "        text_emb /= text_emb.norm(dim=-1, keepdim=True)\n",
    "        text_emb = text_emb.cpu().detach().numpy().astype(\"float32\")[0]\n",
    "    return text_emb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "pos_emb = get_image_emb(\"https://i.imgur.com/75aBao3.jpg\")\n",
    "neg_emb = get_image_emb(\"https://i.imgur.com/DdMvFad.jpg\")\n",
    "# print(tp_emb.shape)\n",
    "\n",
    "vals = clf.predict([pos_emb, neg_emb])\n",
    "print(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[38528.0], [43168.0], [34144.0], [52128.0], [36800.0], [36448.0], [42272.0], [38528.0], [34272.0], [43936.0], [37248.0], [43136.0], [41920.0], [36576.0], [41376.0], [50816.0]]\n",
      "[[33696.0], [37216.0], [61312.0], [44960.0], [38048.0], [34848.0], [40704.0], [40640.0], [38208.0], [38496.0], [47712.0], [42336.0], [37728.0], [44576.0], [36864.0], [35872.0]]\n",
      "[[41536.0], [40128.0], [35008.0], [43936.0], [38848.0], [39488.0], [40704.0], [41184.0], [45920.0], [34016.0], [39712.0], [49984.0], [41184.0], [42016.0], [40864.0], [42176.0]]\n",
      "[[34624.0], [38240.0], [48832.0], [51008.0], [33632.0], [inf], [39936.0], [39264.0], [35968.0], [34720.0], [36576.0], [38176.0], [45408.0], [48864.0], [40480.0], [60448.0]]\n",
      "[[37952.0], [36544.0], [41056.0], [34720.0], [49792.0], [39008.0], [38304.0], [54720.0], [49152.0], [38080.0], [33184.0], [35616.0], [40704.0], [37152.0], [36192.0], [35712.0]]\n",
      "[[37120.0], [35200.0], [47072.0], [40384.0], [36096.0], [39904.0], [39680.0], [40256.0], [40000.0], [38080.0], [47488.0], [36960.0], [35328.0], [33536.0], [33568.0], [36896.0]]\n",
      "[[57824.0], [40320.0], [35456.0], [44800.0], [44480.0], [38624.0], [44416.0], [39264.0], [48384.0], [36064.0], [52128.0], [39424.0], [33664.0], [33568.0], [40160.0], [40512.0]]\n",
      "[[37728.0], [61472.0], [48096.0], [38400.0], [38880.0], [40320.0], [36736.0], [35008.0], [62816.0], [37216.0], [36672.0], [36704.0], [37728.0], [40448.0], [45760.0], [36192.0]]\n",
      "[[40416.0], [46656.0], [49472.0], [38400.0], [35488.0], [43712.0], [41504.0], [37216.0], [41856.0], [43168.0], [38880.0], [41312.0], [46304.0], [38272.0], [40832.0], [36672.0]]\n",
      "[[41856.0], [35424.0], [42336.0], [37728.0], [38112.0], [38944.0], [36992.0], [44992.0], [36992.0], [38816.0], [50272.0], [40832.0], [36032.0], [41856.0], [38304.0], [37696.0]]\n",
      "[[34112.0], [41664.0], [39968.0], [43776.0], [45920.0], [41472.0], [37440.0], [40800.0], [41984.0], [37152.0], [36192.0], [44928.0], [38464.0], [40736.0], [48832.0], [38560.0]]\n",
      "[[37472.0], [43488.0], [36768.0], [38720.0], [46336.0], [43840.0], [35200.0], [38272.0], [41056.0], [47904.0], [39680.0], [39168.0], [38880.0], [54240.0], [40992.0], [37216.0]]\n",
      "[[35904.0], [43680.0], [38016.0], [39232.0], [37536.0], [58336.0], [37408.0], [41472.0], [37664.0], [36032.0], [35200.0], [40480.0], [40384.0], [43200.0], [36160.0], [34816.0]]\n",
      "[[47904.0], [36992.0], [58592.0], [36512.0], [34912.0], [37056.0], [42016.0], [45024.0], [36960.0], [42656.0], [44896.0], [49760.0], [39744.0], [36736.0], [38784.0], [40288.0]]\n",
      "[[48064.0], [47136.0], [46688.0], [37632.0], [38560.0], [42688.0], [39104.0], [45152.0], [39264.0], [34656.0], [38144.0], [40192.0], [38848.0], [54400.0], [37856.0], [39328.0]]\n",
      "[[37376.0], [42688.0], [36160.0], [39264.0], [41120.0], [37184.0], [40736.0], [41088.0], [35712.0], [36512.0], [42464.0], [34240.0], [42208.0], [42176.0], [37920.0], [55808.0]]\n",
      "[[51264.0], [36320.0], [43072.0], [42176.0], [34816.0], [55552.0], [37760.0], [42016.0], [35776.0], [34368.0], [40128.0], [41472.0], [51328.0], [41120.0], [35488.0], [48480.0]]\n",
      "[[39840.0], [43488.0], [40256.0], [36672.0], [40032.0], [37664.0], [32048.0], [41120.0], [45504.0], [37312.0], [43776.0], [47648.0], [52064.0], [38304.0], [51072.0], [35328.0]]\n",
      "[[34368.0], [46016.0], [33024.0], [41664.0], [42240.0], [42976.0], [35968.0], [37216.0], [41152.0], [41952.0], [44800.0], [41568.0], [55872.0], [41344.0], [37792.0], [35456.0]]\n",
      "[[34848.0], [36768.0], [41440.0], [38304.0], [40896.0], [36256.0], [37344.0], [37312.0], [38656.0], [36640.0], [42464.0], [35488.0], [37408.0], [37408.0], [60736.0], [43744.0]]\n",
      "[[43296.0], [55104.0], [44576.0], [47872.0], [35936.0], [43104.0], [35456.0], [41504.0], [34656.0], [46944.0], [40384.0], [36064.0], [39136.0], [36032.0], [55232.0], [38560.0]]\n",
      "[[36992.0], [40416.0], [37664.0], [40160.0], [37696.0], [37280.0], [41184.0], [43392.0], [57280.0], [38240.0], [38272.0], [41440.0], [37504.0], [40384.0], [41376.0], [42912.0]]\n",
      "[[46528.0], [34240.0], [49984.0], [52768.0], [44480.0], [42624.0], [38912.0], [50624.0], [37472.0], [45184.0], [46368.0], [40288.0], [36928.0], [41696.0], [43200.0], [36448.0]]\n",
      "[[36864.0], [41824.0], [41280.0], [36416.0], [36480.0], [39552.0], [36544.0], [56352.0], [48064.0], [38880.0], [45632.0], [37152.0], [42336.0], [47744.0], [34048.0], [40864.0]]\n",
      "[[42464.0], [39904.0], [39360.0], [36832.0], [37696.0], [36608.0], [39008.0], [39456.0], [47616.0], [34432.0], [42528.0], [50624.0], [39520.0], [37792.0], [37568.0], [37312.0]]\n",
      "[[37440.0], [37440.0], [34592.0], [36992.0], [38432.0], [44832.0], [41952.0], [42720.0], [41056.0], [36960.0], [36896.0], [40512.0], [53344.0], [36160.0], [39808.0], [42560.0]]\n",
      "[[63040.0], [41760.0], [59648.0], [40576.0], [42624.0], [41952.0], [39488.0], [52768.0], [43712.0], [55808.0], [38208.0], [44512.0], [38880.0], [52256.0], [48128.0], [53248.0]]\n",
      "[[43040.0], [45632.0], [39904.0], [39424.0], [38816.0], [38912.0], [34496.0], [39040.0], [41184.0], [36544.0], [43264.0], [46048.0], [37440.0], [37376.0], [36896.0], [37856.0]]\n",
      "[[42624.0], [35680.0], [39136.0], [43872.0], [39680.0], [36256.0], [47488.0], [38528.0], [36800.0], [44224.0], [52096.0], [32864.0], [46976.0], [38272.0], [39968.0], [37920.0]]\n",
      "[[37216.0], [38304.0], [45600.0], [41536.0], [38240.0], [39296.0], [42368.0], [36320.0], [41088.0], [50464.0], [45152.0], [49536.0], [43904.0], [40320.0], [44672.0], [38240.0]]\n",
      "[[40704.0], [40832.0], [54240.0], [40608.0], [43872.0], [46112.0], [42464.0], [51680.0], [38272.0], [35360.0], [39008.0], [47232.0], [39776.0], [45472.0], [36032.0], [37696.0]]\n",
      "[[37152.0], [40736.0], [38144.0], [39904.0], [35840.0], [39968.0], [34432.0], [35552.0], [43744.0], [33344.0], [35776.0], [38720.0], [44320.0], [34912.0], [51136.0], [58976.0]]\n",
      "[[37216.0], [41984.0], [36000.0], [42048.0], [40800.0], [38560.0], [35680.0], [47392.0], [32112.0], [57728.0], [47488.0], [37888.0], [45920.0], [49472.0], [40320.0], [38336.0]]\n",
      "[[38208.0], [35360.0], [42272.0], [42464.0], [45280.0], [41472.0], [44320.0], [inf], [47392.0], [36544.0], [39424.0], [36832.0], [38048.0], [37792.0], [41888.0], [43072.0]]\n",
      "[[41888.0], [39200.0], [41856.0], [39904.0], [37152.0], [43840.0], [37728.0], [37088.0], [39200.0], [40704.0], [36992.0], [38336.0], [41408.0], [38848.0], [39104.0], [57184.0]]\n",
      "[[47296.0], [43520.0], [42496.0], [34720.0], [37088.0], [58784.0], [38496.0], [37440.0], [37792.0], [40832.0], [39232.0], [41536.0], [52352.0], [37728.0], [35872.0], [53312.0]]\n",
      "[[36000.0], [46752.0], [36192.0], [38912.0], [34400.0], [35584.0], [34048.0], [43360.0], [38976.0], [36320.0], [33760.0], [38528.0], [37376.0], [38368.0], [37536.0], [41280.0]]\n",
      "[[36224.0], [38112.0], [40704.0], [38880.0], [42976.0], [39808.0], [51648.0], [40160.0], [64832.0], [48384.0], [42528.0], [35328.0], [49664.0], [45056.0], [38048.0], [36576.0]]\n",
      "[[42464.0], [39968.0], [37184.0], [41536.0], [38624.0], [40288.0], [41152.0], [35712.0], [38080.0], [55680.0], [37600.0], [41216.0], [inf], [40416.0], [51072.0], [41216.0]]\n",
      "[[40416.0], [40224.0], [52416.0], [35200.0], [43520.0], [39456.0], [38080.0], [40800.0], [47872.0], [36192.0], [44704.0], [39168.0], [44832.0], [35104.0], [45728.0], [34144.0]]\n",
      "[[36768.0], [38720.0], [39776.0], [38432.0], [36416.0], [36832.0], [40576.0], [35712.0], [40256.0], [39328.0], [39392.0], [38240.0], [39072.0], [40928.0], [34432.0], [40160.0]]\n",
      "[[39008.0], [38720.0], [38656.0], [36800.0], [58720.0], [38976.0], [59808.0], [46272.0], [35712.0], [42208.0], [42240.0], [41472.0], [38880.0], [40384.0], [40544.0], [34976.0]]\n",
      "[[39680.0], [35968.0], [40736.0], [inf], [39936.0], [51008.0], [37376.0], [41120.0], [35712.0], [40576.0], [40096.0], [42432.0], [41216.0], [39264.0], [44480.0], [38080.0]]\n",
      "[[38656.0], [41344.0], [40768.0], [55040.0], [39968.0], [40736.0], [44096.0], [38112.0], [52032.0], [35360.0], [34752.0], [35968.0], [35840.0], [33280.0], [42368.0], [61952.0]]\n",
      "[[40672.0], [37312.0], [39168.0], [41248.0], [37344.0], [41344.0], [39648.0], [41600.0], [39360.0], [35168.0], [36256.0], [41504.0], [35488.0], [48352.0], [43456.0], [46336.0]]\n",
      "[[40768.0], [35680.0], [38336.0], [39168.0], [35712.0], [52800.0], [37472.0], [41856.0], [35328.0], [36736.0], [35328.0], [37856.0], [42528.0], [38272.0], [49728.0], [46976.0]]\n",
      "[[35136.0], [35584.0], [38656.0], [42720.0], [37856.0], [36224.0], [32800.0], [46208.0], [36896.0], [47712.0], [44736.0], [46528.0], [62912.0], [37376.0], [37824.0], [41632.0]]\n",
      "[[42368.0], [45600.0], [41024.0], [43552.0], [40224.0], [36192.0], [48096.0], [35968.0], [39904.0], [36928.0], [58176.0], [45280.0], [40864.0], [40192.0], [42464.0], [39872.0]]\n",
      "[[49920.0], [40480.0], [43968.0], [47104.0], [46656.0], [36640.0], [45280.0], [36640.0], [36544.0], [48448.0], [40928.0], [34976.0], [51552.0], [41920.0], [40032.0], [45152.0]]\n",
      "[[51232.0], [34336.0], [46720.0], [41344.0], [42208.0], [41952.0], [42464.0], [52288.0], [39776.0], [40992.0], [52064.0], [42752.0], [42464.0], [49568.0], [37760.0], [50752.0]]\n",
      "[[35968.0], [44064.0], [43168.0], [41760.0], [42944.0], [42400.0], [52256.0], [46848.0], [41920.0], [37888.0], [36480.0], [43328.0], [52736.0], [41248.0], [39136.0], [36832.0]]\n",
      "[[43808.0], [44064.0], [38368.0], [42656.0], [41152.0], [41408.0], [47232.0], [40416.0], [41536.0], [48672.0], [45664.0], [43104.0], [46496.0], [43904.0], [41376.0], [48768.0]]\n",
      "[[40192.0], [47168.0], [40192.0], [59328.0], [42368.0], [37184.0], [38112.0], [45024.0], [39392.0], [40768.0], [38624.0], [37152.0], [42016.0], [47648.0], [39712.0], [44672.0]]\n",
      "[[48704.0], [41728.0], [38240.0], [38944.0], [48800.0], [43392.0], [36320.0], [36928.0], [41504.0], [37824.0], [40224.0], [43104.0], [36032.0], [42432.0], [35168.0], [43488.0]]\n",
      "[[50944.0], [39136.0], [41440.0], [50208.0], [61568.0], [49504.0], [40320.0], [44288.0], [45984.0], [53984.0], [48000.0], [54816.0], [48736.0], [45504.0], [35584.0], [43616.0]]\n",
      "[[38432.0], [41120.0], [38848.0], [38944.0], [44256.0], [40768.0], [37920.0], [54208.0], [36192.0], [40320.0], [35552.0], [42912.0], [37088.0], [43968.0], [38816.0], [41504.0]]\n",
      "[[35808.0], [41184.0], [38848.0], [43584.0], [45152.0], [41344.0], [43104.0], [36288.0], [39424.0], [39520.0], [37216.0], [64256.0], [38432.0], [36896.0], [39104.0], [43680.0]]\n",
      "[[55104.0], [41152.0], [46368.0], [45184.0], [46304.0], [47904.0], [41632.0], [39424.0], [38464.0], [38624.0], [47008.0], [38816.0], [36032.0], [57760.0], [43872.0], [37952.0]]\n",
      "[[47072.0], [42592.0], [41824.0], [39360.0], [51552.0], [44736.0], [41152.0], [46656.0], [35712.0], [37280.0], [49088.0], [42368.0], [39008.0], [40576.0], [38048.0], [39392.0]]\n",
      "[[43712.0], [40256.0], [37088.0], [39488.0], [45664.0], [37504.0], [38208.0], [45152.0], [39968.0], [60288.0], [45152.0], [40736.0], [49472.0], [38976.0], [46240.0], [48928.0]]\n",
      "[[38816.0], [42272.0], [50304.0], [42656.0], [41120.0], [40480.0], [38304.0], [38752.0], [42208.0], [44192.0], [46656.0], [42400.0], [41664.0], [37280.0], [43680.0], [44768.0]]\n",
      "[[44160.0], [45504.0], [42208.0], [44160.0], [39776.0], [40768.0], [41152.0], [43424.0], [42688.0], [44704.0], [47776.0], [40160.0], [43808.0], [41792.0], [36160.0], [39200.0]]\n",
      "[[39392.0], [43072.0], [44000.0], [41312.0], [53792.0], [43328.0], [35904.0], [38048.0], [39808.0], [42080.0], [37920.0], [45184.0], [49856.0], [41856.0], [47072.0], [39424.0]]\n",
      "[[46720.0], [37632.0], [53408.0], [58240.0], [41408.0], [41152.0], [42656.0], [51360.0], [34784.0], [43488.0], [38144.0], [35712.0], [43552.0], [37120.0], [35104.0], [44448.0]]\n",
      "[[36576.0], [51936.0], [41952.0], [38720.0], [48480.0], [35616.0], [42720.0], [47264.0], [38880.0], [45344.0], [42240.0], [37152.0], [36928.0], [45344.0], [43392.0], [43456.0]]\n",
      "[[41408.0], [56352.0], [44704.0], [36032.0], [48160.0], [43904.0], [40864.0], [37920.0], [45824.0], [42656.0], [42272.0], [53888.0], [43520.0], [42816.0], [41120.0], [57888.0]]\n",
      "[[46912.0], [36512.0], [40256.0], [37344.0], [41120.0], [39072.0], [42560.0], [36000.0], [43776.0], [40480.0], [39616.0], [41344.0], [46080.0], [39168.0], [40896.0], [40832.0]]\n",
      "[[inf], [41952.0], [40192.0], [44896.0], [44096.0], [62688.0], [34560.0], [44448.0], [40608.0], [48480.0], [44384.0], [36960.0], [41536.0], [43776.0], [43840.0], [43328.0]]\n",
      "[[39520.0], [46880.0], [39776.0], [47200.0], [37728.0], [42976.0], [46624.0], [43392.0], [43904.0], [45472.0], [36800.0], [43840.0], [37760.0], [38368.0], [49504.0], [47488.0]]\n",
      "[[44032.0], [42656.0], [43264.0], [43424.0], [43776.0], [49312.0], [36928.0], [49216.0], [46176.0], [37088.0], [45472.0], [46848.0], [45216.0], [49152.0], [39904.0], [54240.0]]\n",
      "[[49632.0], [40544.0], [42784.0], [38624.0], [40128.0], [44704.0], [39936.0], [37760.0], [41152.0], [39520.0], [37184.0], [39840.0], [37408.0], [39936.0], [41920.0], [42688.0]]\n",
      "[[41664.0], [41824.0], [43136.0], [43424.0], [47136.0], [39296.0], [41632.0], [41120.0], [43776.0], [52160.0], [54592.0], [41792.0], [51424.0], [44800.0], [43872.0], [39360.0]]\n",
      "[[41056.0], [38016.0], [65184.0], [61984.0], [42048.0], [46528.0], [42080.0], [43584.0], [56768.0], [42176.0], [40800.0], [36832.0], [42400.0], [36544.0], [51488.0], [49280.0]]\n",
      "[[41856.0], [41344.0], [36896.0], [48992.0], [45600.0], [43104.0], [38656.0], [37216.0], [40192.0], [49024.0], [40096.0], [37440.0], [37536.0], [46304.0], [44480.0], [46624.0]]\n",
      "[[41216.0], [37696.0], [41984.0], [55520.0], [49728.0], [43136.0], [39712.0], [38464.0], [36928.0], [35776.0], [39936.0], [49920.0], [52480.0], [44192.0], [49024.0], [52544.0]]\n",
      "[[35872.0], [43264.0], [37184.0], [38720.0], [38624.0], [35584.0], [46528.0], [35200.0], [36832.0], [43840.0], [56864.0], [39232.0], [47520.0], [37152.0], [45248.0], [35584.0]]\n",
      "[[42144.0], [39392.0], [41856.0], [41824.0], [41600.0], [36800.0], [38304.0], [38784.0], [35072.0], [38208.0], [37376.0], [38176.0], [43008.0], [42624.0], [47808.0], [35904.0]]\n",
      "[[52576.0], [38464.0], [37216.0], [47424.0], [47456.0], [38528.0], [37536.0], [44416.0], [45344.0], [42080.0], [36064.0], [42656.0], [39232.0], [42240.0], [39040.0], [37248.0]]\n",
      "[[40576.0], [40224.0], [37312.0], [41024.0], [47296.0], [41952.0], [40032.0], [44256.0], [42656.0], [44896.0], [41344.0], [39552.0], [38720.0], [40000.0], [35936.0], [46368.0]]\n",
      "[[35776.0], [36928.0], [inf], [43072.0], [47136.0], [41568.0], [39552.0], [42432.0], [39968.0], [45376.0], [41728.0], [39360.0], [45760.0], [39424.0], [46336.0], [44672.0]]\n",
      "[[44992.0], [inf], [36064.0], [42016.0], [44128.0], [52448.0], [43904.0], [39360.0], [52640.0], [42656.0], [44992.0], [46240.0], [43744.0], [40000.0], [38400.0], [44672.0]]\n",
      "[[45600.0], [38560.0], [56512.0], [50208.0], [43776.0], [38432.0], [49056.0], [38464.0], [38240.0], [44896.0], [50368.0], [38208.0], [49248.0], [42848.0], [40928.0], [40704.0]]\n",
      "[[44096.0], [42304.0], [36704.0], [47104.0], [37728.0], [40544.0], [39520.0], [43744.0], [44736.0], [48864.0], [41088.0], [39008.0], [50528.0], [44256.0], [34528.0], [35552.0]]\n",
      "[[44320.0], [38496.0], [36832.0], [45696.0], [44992.0], [39712.0], [51776.0], [47264.0], [39040.0], [40352.0], [42112.0], [41376.0], [42912.0], [40960.0], [45088.0], [41248.0]]\n",
      "[[41664.0], [39232.0], [38240.0], [36928.0], [38464.0], [36288.0], [42624.0], [38304.0], [40576.0], [51392.0], [45408.0], [38464.0], [36000.0], [46880.0], [40032.0], [41920.0]]\n",
      "[[39872.0], [42112.0], [42048.0], [46368.0], [40384.0], [41760.0], [47488.0], [43232.0], [41408.0], [39296.0], [37792.0], [41056.0], [51296.0], [39840.0], [36960.0], [55520.0]]\n",
      "[[40096.0], [38784.0], [39584.0], [43200.0], [49248.0], [41536.0], [43840.0], [52000.0], [38528.0], [46176.0], [47296.0], [35648.0], [40512.0], [42080.0], [39584.0], [40640.0]]\n",
      "[[51488.0], [38208.0], [40832.0], [50336.0], [43424.0], [47808.0], [38240.0], [41024.0], [40704.0], [40128.0], [43392.0], [38144.0], [37632.0], [38016.0], [41600.0], [40288.0]]\n",
      "[[37408.0], [46176.0], [39136.0], [37600.0], [45856.0], [43424.0], [45408.0], [37344.0], [50400.0], [45216.0], [38016.0], [51296.0], [44640.0], [47456.0], [44672.0], [44352.0]]\n",
      "[[34848.0], [48800.0], [44832.0], [43072.0], [34944.0], [46496.0], [37696.0], [39296.0], [37888.0], [35968.0], [44864.0], [40032.0], [40576.0], [39808.0], [39072.0], [42304.0]]\n",
      "[[41376.0], [44736.0], [48224.0], [40736.0], [37600.0], [43776.0], [54432.0], [55584.0], [44288.0], [39776.0], [37312.0], [45216.0], [41888.0], [42304.0], [49024.0], [34368.0]]\n",
      "[[42432.0], [49152.0], [42208.0], [43072.0], [44384.0], [37472.0], [45088.0], [42688.0], [41152.0], [39104.0], [44608.0], [49536.0], [50208.0], [41344.0], [54112.0], [39232.0]]\n",
      "[[38912.0], [40576.0], [34528.0], [43136.0], [46400.0], [39936.0], [39232.0], [54080.0], [34112.0], [40544.0], [43520.0], [40576.0], [46976.0], [38816.0], [36256.0], [34688.0]]\n",
      "[[41280.0], [39392.0], [44160.0], [39392.0], [44928.0], [54912.0], [47648.0], [35680.0], [47072.0], [36736.0], [45856.0], [36128.0], [43232.0], [40480.0], [50656.0], [37824.0]]\n",
      "[[43936.0], [36992.0], [42784.0], [50720.0], [42528.0], [62720.0], [43200.0], [40608.0], [47136.0], [37024.0], [42656.0], [38176.0], [41280.0], [43392.0], [37280.0], [39232.0]]\n",
      "[[41056.0], [37696.0], [39104.0], [43552.0], [54752.0], [36160.0], [45152.0], [40768.0], [40672.0], [38080.0], [38496.0], [51488.0]]\n"
     ]
    }
   ],
   "source": [
    "pos_x_val = torch.from_numpy(np.load(r\"F:\\nsfw_testset\\drawings-test\\img_emb\\img_emb_0.npy\"))\n",
    "neg_x_val = torch.from_numpy(np.load(r\"F:\\nsfw_testset\\neutral-test\\img_emb\\img_emb_0.npy\"))\n",
    "\n",
    "x_val, y_val = npy_to_tensor_dataset(pos_x_val, neg_x_val)\n",
    "val_dataset = TensorDataset(x_val, y_val)\n",
    "\n",
    "# Define the data loaders\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=16)\n",
    "model.eval()\n",
    "with torch.cuda.amp.autocast():\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            # Forward pass\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # print(inputs.tolist(), labels.tolist())\n",
    "            outputs = model(inputs)\n",
    "            outputs_list = outputs.tolist()\n",
    "            print(outputs_list)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            # print(loss)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
