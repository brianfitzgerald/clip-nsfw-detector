{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install clip-retrieval img2dataset pandas numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu117Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torchvision in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (0.14.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (4.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (1.24.3)\n",
      "Requirement already satisfied: requests in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (2.29.0)\n",
      "Requirement already satisfied: torch==1.13.1 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (1.13.1+cu117)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from torchvision) (9.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from requests->torchvision) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from requests->torchvision) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\coold\\documents\\github\\clip-nsfw-detector\\.venv\\lib\\site-packages (from requests->torchvision) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision>=0.10.1,<2 --index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\coold\\Documents\\GitHub\\clip-nsfw-detector\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from clip_retrieval import clip_inference\n",
    "import shutil\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n",
    "base_folder = os.path.join(\"F:\", \"nude_sexy_safe_v1_x320\", \"testing\")\n",
    "output_base_folder = os.path.join(\"F:\", \"nsfw_embeddings\")\n",
    "categories = [\"nude\", \"sexy\", \"safe\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of samples has been estimated to be 3878\n",
      "Starting the worker\n",
      "dataset is 37\n",
      "Starting work on task 0\n",
      "warming up with batch size 32 on cuda\n",
      "done warming up in 3.021339178085327s\n",
      "The number of samples has been estimated to be 2121\n",
      "Starting the worker\n",
      "dataset is 37\n",
      "Starting work on task 0\n",
      "The number of samples has been estimated to be 4050\n",
      "Starting the worker\n",
      "dataset is 37\n",
      "Starting work on task 0\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(categories):\n",
    "    folder = os.path.join(base_folder, c)\n",
    "    output_folder = os.path.join(output_base_folder, c)\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.mkdir(output_folder)\n",
    "    # https://github.com/rom1504/clip-retrieval/issues/220\n",
    "    clip_inference(folder, output_folder, num_prepro_workers=0, clip_model=\"ViT-L/14\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_embeddings = {}\n",
    "for c in categories:\n",
    "    cat_embeddings[c] = np.load(os.path.join(output_base_folder, c, \"img_emb\", \"img_emb_0.npy\"))\n",
    "\n",
    "pos_x = np.concatenate([cat_embeddings[\"nude\"], cat_embeddings[\"sexy\"]])\n",
    "neg_x = cat_embeddings[\"safe\"]\n",
    "\n",
    "num_pos = pos_x.shape[0]\n",
    "num_neg = neg_x.shape[0]\n",
    "\n",
    "pos_y = np.ones((num_pos, 1))\n",
    "neg_y = np.zeros((num_neg, 1))\n",
    "\n",
    "x_np = np.vstack([pos_x, neg_x])\n",
    "y_np = np.vstack([pos_y, neg_y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16 torch.float16\n"
     ]
    }
   ],
   "source": [
    "x = torch.from_numpy(x_np).type(torch.float16)\n",
    "y = torch.from_numpy(y_np).type(torch.float16)\n",
    "print(x.dtype, y.dtype)\n",
    "train_dataset = TensorDataset(x, y)\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([853, 768])\n"
     ]
    }
   ],
   "source": [
    "baseline = torch.from_numpy(np.load(r\"F:\\nsfw_testset\\nsfw_testset\\drawings-test\\img_emb\\img_emb_0.npy\"))\n",
    "print(baseline.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 315/315 [00:01<00:00, 194.43it/s, epoch=0, loss=0.00276]\n",
      "100%|██████████| 315/315 [00:01<00:00, 214.61it/s, epoch=1, loss=0.869]\n",
      "100%|██████████| 315/315 [00:01<00:00, 215.54it/s, epoch=2, loss=0.00286]\n",
      "100%|██████████| 315/315 [00:01<00:00, 213.99it/s, epoch=3, loss=0.903]\n",
      "100%|██████████| 315/315 [00:01<00:00, 216.79it/s, epoch=4, loss=0.00342]\n",
      "100%|██████████| 315/315 [00:01<00:00, 214.60it/s, epoch=5, loss=0.0031]\n",
      "100%|██████████| 315/315 [00:01<00:00, 190.22it/s, epoch=6, loss=0.00337]\n",
      "100%|██████████| 315/315 [00:01<00:00, 207.56it/s, epoch=7, loss=0.00326]\n",
      "100%|██████████| 315/315 [00:01<00:00, 212.10it/s, epoch=8, loss=0.88] \n",
      "100%|██████████| 315/315 [00:01<00:00, 212.05it/s, epoch=9, loss=0.891]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "class H14_NSFW_Detector(nn.Module):\n",
    "    def __init__(self, input_size=1024):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(self.input_size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 16),\n",
    "            nn.Linear(16, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = H14_NSFW_Detector(768).to(device)\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=1e-10)\n",
    "num_epochs = 10\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "\n",
    "with torch.cuda.amp.autocast():\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        train_iter = tqdm(train_loader, total=len(train_loader))\n",
    "        for inputs, labels in train_iter:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Clear gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_iter.set_postfix(loss=loss.item(), epoch=epoch)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
